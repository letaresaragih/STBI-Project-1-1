# -*- coding: utf-8 -*-
"""articles_data_word2vec_tf_idf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rggmQcHCJ4Ui3VF9pEvBIM9xWwoaG4SC

# Dataset 1: articles_dataset.csv

#### a. Import required libraries
"""

from google.colab import drive
drive.mount('/content/drive')

#import library
import os
import random
import re
import string

import nltk
import numpy as np
import pandas as pd

from gensim.models import Word2Vec

from nltk import word_tokenize
from nltk.corpus import stopwords
from collections import Counter

nltk.download("stopwords")

SEED = 42
random.seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)
np.random.seed(SEED)

"""#### b. Data Preprocessing"""

import pandas as pd
df = pd.read_csv("./drive/MyDrive/STBI/Proyek/Dataset/articles_data.csv")
df.head()

df.shape

df.info()

# drop atribut 
df = df.drop(['Unnamed: 0', 'source_id', 'title', 'description', 'source_name', 'author', 'url', 'url_to_image', 'published_at', 'top_article', 'engagement_reaction_count', 'engagement_comment_count', 'engagement_share_count', 'engagement_comment_plugin_count'], axis = 1)

df.info()

# Checking duplicate value
print(str(df.describe(include=object)))

# remove duplicate value (text) in contenct attribute
df.drop_duplicates(subset=['content'], keep='last')

# Checking null value in attribute 
df.isna().sum()

#drop null value
data = df.dropna()

df

#saving the result
data.to_csv("./drive/MyDrive/STBI/Proyek/Dataset/clean_articles_data_word2vec.csv", index=True)

"""#### C. Text Preprocessing"""

# remove punctuation
def punctuation(txt):
  return re.sub(r"[^\w\s]","", str(txt))

data['content'] = data['content'].apply(punctuation)

import nltk
nltk.download('punkt')
# tokenization
def word_tokenize_wrapper(text):
  return word_tokenize(text)
data['content'] = data['content'].apply(word_tokenize_wrapper)

#stopword removal
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')
print(sw_nltk)
def stopword(text):
  words = [word for word in text if word.lower() not in sw_nltk]
  return words
data['content'] = data['content'].apply(stopword)

#normalization
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
def lemma(text):
  lemmatizer = WordNetLemmatizer()
  Output= [lemmatizer.lemmatize(words_sent) for words_sent in text]
  return Output
data['content'] = data['content'].apply(lemma)

#saving the rusult
data.to_csv("./drive/MyDrive/STBI/Proyek/Dataset/prepro_articles_data_word2vec.csv", index=True)

data

"""#### D. TF-IDF"""

# untuk tf-idf
import pandas as pd
data = pd.read_csv("./drive/MyDrive/STBI/Proyek/Dataset/prepro_articles_data_word2vec.csv")
data.head()

from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf(df):
    tfidf = TfidfVectorizer( stop_words='english',use_idf=True)
    tfidf_matrix = tfidf.fit_transform(df)
    return tfidf_matrix

# Let's create a matrix with tfidf for the column abstract
tfidf_matrix = tfidf(data['content'].values.astype('U'))

# in order to explore which documents have more similar respresentaiton, consine simliartiy can be used
from sklearn.metrics.pairwise import linear_kernel
cosine_similarities = linear_kernel(tfidf_matrix[0:1], tfidf_matrix).flatten()

# 10 most related documents indices
related_docs_indices = cosine_similarities.argsort()[:-11:-1]
print("Related Document:",related_docs_indices)

# Cosine Similarties of related documents
print("Cosine Similarites of related documents",cosine_similarities[related_docs_indices])

# Let's take a look at two most similar document
data.iloc[1]['content']

from wordcloud import WordCloud
import matplotlib.pyplot as plt
wordcloud = WordCloud().generate(data.iloc[0]['content'])
plt.imshow(wordcloud, interpolation="bilinear")

"""#### E. Evalution"""

def AveragePrecission(data):
    hasil = 0
    k = 1
    for i in range(len(data)):
        hasil +=  (k / data[i-1].get("content"))
        k += 1   
    if len(dict) > 0:
        return hasil/len(data)
    else:
        return 0
            
    

def MeanAveragePrecission():
    ct = 0
    ans = 0
    jumlah_result = 10
    for i in range (10):
        ct = result[i-1]
        ans += AveragePrecission(ct)
        print("result :", i, "\tAverage Precission :", AveragePrecission(ct))

    print("\nTotal Mean Average Precission :", ans/jumlah_result)