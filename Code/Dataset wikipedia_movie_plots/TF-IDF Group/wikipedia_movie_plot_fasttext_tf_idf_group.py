# -*- coding: utf-8 -*-
"""Wikipedia Movie Plots Fasttext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wZNG8o5KCH_07FpbRP3S2Stp6K18TY6f
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
# import numpy as np 
import re
import string 
import nltk
nltk.download('punkt')
# import word_tokenize & FreqDist from NLTK
from nltk.tokenize import word_tokenize 
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

df = pd.read_csv("./drive/MyDrive/Semester8/STBI/wiki_movie_plots_deduped.csv", index_col=0)

df.info

df.describe(include=object)

df.isna().sum()

result = df.drop(['Origin/Ethnicity', 'Director', 'Cast', 'Wiki Page'], axis = 1)

result

print(str(result.describe(include=object)))

result =result.drop_duplicates('Plot')
result.describe()

# remove number
def remove_number(text):
    return  re.sub(r"\[0-9]+", "", str(text))
 
result['Plot'] = result['Plot'].apply(remove_number)

# remove punctuation
def punctuation(txt):
  return re.sub(r"[^\w\s]","", str(txt))

result['Plot'] = result['Plot'].apply(punctuation)

# tokenization
def word_tokenize_wrapper(text):
  return word_tokenize(text)
result = result['Plot'].apply(word_tokenize_wrapper)

#stopword removal
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')
print(sw_nltk)
def stopword(text):
  words = [word for word in text if word.lower() not in sw_nltk]
  return words
result = result.apply(stopword)

result

#normalization
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
def lemma(text):
  lemmatizer = WordNetLemmatizer()
  Output= [lemmatizer.lemmatize(words_sent) for words_sent in text]
  return Output
result = result.apply(lemma)

result.head()

#word embedding
from gensim.models import FastText
model = FastText(result,  window=5,  min_count=5,   sg=1 ,size = 200)

fasttext = model.wv

fasttext.similar_by_word("moon", topn=5)

fasttext.get_vector('moon')

from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf(data):
    tfidf = TfidfVectorizer( stop_words='english',use_idf=True)
    tfidf_matrix = tfidf.fit_transform(data)
    return tfidf_matrix

# Let's create a matrix with tfidf for the column abstract
tfidf_matrix = tfidf(df['Plot'])

# in order to explore which documents have more similar respresentaiton, consine simliartiy can be used
from sklearn.metrics.pairwise import linear_kernel
cosine_similarities = linear_kernel(tfidf_matrix[0:1], tfidf_matrix).flatten()

# 10 most related documents indices
related_docs_indices = cosine_similarities.argsort()[:-11:-1]
print("Related Document:",related_docs_indices)

# Cosine Similarties of related documents
print("Cosine Similarites of related documents",cosine_similarities[related_docs_indices])

df.iloc[0]['Plot']

from wordcloud import WordCloud
import matplotlib.pyplot as plt
wordcloud = WordCloud().generate(df.iloc[0]['Plot'])
plt.imshow(wordcloud, interpolation="bilinear")

# importing KMeans library of sklearn
from sklearn.cluster import KMeans

def kmeans(n_clusters):
    kmean_model = KMeans(n_clusters = n_clusters,random_state=0)
    return kmean_model

import gensim
from gensim.models import Doc2Vec

def doc2vec():
    document_tagged = []
    tagged_count = 0
    for _ in df['Plot'].values:
        document_tagged.append(gensim.models.doc2vec.TaggedDocument(_,[tagged_count]))
        tagged_count +=1 
    d2v = Doc2Vec(document_tagged)
    d2v.train(document_tagged,epochs=d2v.epochs,total_examples=d2v.corpus_count)
    return d2v.docvecs.vectors_docs

from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf(result):
    tfidf = TfidfVectorizer( stop_words='english',use_idf=True)
    tfidf_matrix = tfidf.fit_transform(result)
    return tfidf_matrix

# Let's create a matrix with tfidf for the column abstract
tfidf_matrix = tfidf(df['Plot'].values.astype('U'))

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

nc = range(1,10)
kmeans = [KMeans(n_clusters = i, n_init = 100, max_iter = 500) for i in nc]
score = [kmeans[i].fit(tfidf_matrix).score(tfidf_matrix) for i in range(len(kmeans))]
plt.plot(nc,score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()